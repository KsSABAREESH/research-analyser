[INFO|2025-07-15 09:36:11] tokenization_utils_base.py:2023 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/tokenizer.model
[INFO|2025-07-15 09:36:11] tokenization_utils_base.py:2023 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/tokenizer.json
[INFO|2025-07-15 09:36:11] tokenization_utils_base.py:2023 >> loading file added_tokens.json from cache at None
[INFO|2025-07-15 09:36:11] tokenization_utils_base.py:2023 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/special_tokens_map.json
[INFO|2025-07-15 09:36:11] tokenization_utils_base.py:2023 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/tokenizer_config.json
[INFO|2025-07-15 09:36:11] tokenization_utils_base.py:2023 >> loading file chat_template.jinja from cache at None
[INFO|2025-07-15 09:36:12] configuration_utils.py:698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/config.json
[INFO|2025-07-15 09:36:12] configuration_utils.py:770 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|2025-07-15 09:36:12] tokenization_utils_base.py:2023 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/tokenizer.model
[INFO|2025-07-15 09:36:12] tokenization_utils_base.py:2023 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/tokenizer.json
[INFO|2025-07-15 09:36:12] tokenization_utils_base.py:2023 >> loading file added_tokens.json from cache at None
[INFO|2025-07-15 09:36:12] tokenization_utils_base.py:2023 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/special_tokens_map.json
[INFO|2025-07-15 09:36:12] tokenization_utils_base.py:2023 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/tokenizer_config.json
[INFO|2025-07-15 09:36:12] tokenization_utils_base.py:2023 >> loading file chat_template.jinja from cache at None
[INFO|2025-07-15 09:36:12] logging.py:143 >> Add pad token: </s>
[INFO|2025-07-15 09:36:12] logging.py:143 >> Loading dataset scientific_equation_dataset_full_30.json...
[INFO|2025-07-15 09:36:18] configuration_utils.py:698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/config.json
[INFO|2025-07-15 09:36:18] configuration_utils.py:770 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|2025-07-15 09:36:18] logging.py:143 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|2025-07-15 09:36:18] logging.py:143 >> KV cache is disabled during training.
[INFO|2025-07-15 09:36:19] modeling_utils.py:1151 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/model.safetensors.index.json
[INFO|2025-07-15 09:37:31] modeling_utils.py:2241 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|2025-07-15 09:37:31] configuration_utils.py:1135 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

[INFO|2025-07-15 09:38:53] modeling_utils.py:5131 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|2025-07-15 09:38:53] modeling_utils.py:5139 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-v0.1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|2025-07-15 09:38:53] configuration_utils.py:1090 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/generation_config.json
[INFO|2025-07-15 09:38:53] configuration_utils.py:1135 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

[INFO|2025-07-15 09:38:53] logging.py:143 >> Gradient checkpointing enabled.
[INFO|2025-07-15 09:38:53] logging.py:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-07-15 09:38:53] logging.py:143 >> Upcasting trainable params to float32.
[INFO|2025-07-15 09:38:53] logging.py:143 >> Fine-tuning method: LoRA
[INFO|2025-07-15 09:38:53] logging.py:143 >> Found linear modules: gate_proj,down_proj,q_proj,o_proj,up_proj,k_proj,v_proj
[INFO|2025-07-15 09:38:54] logging.py:143 >> trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
[INFO|2025-07-15 09:38:54] trainer.py:756 >> Using auto half precision backend
[INFO|2025-07-15 09:38:54] trainer.py:2409 >> ***** Running training *****
[INFO|2025-07-15 09:38:54] trainer.py:2410 >>   Num examples = 30
[INFO|2025-07-15 09:38:54] trainer.py:2411 >>   Num Epochs = 3
[INFO|2025-07-15 09:38:54] trainer.py:2412 >>   Instantaneous batch size per device = 2
[INFO|2025-07-15 09:38:54] trainer.py:2415 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|2025-07-15 09:38:54] trainer.py:2416 >>   Gradient Accumulation steps = 8
[INFO|2025-07-15 09:38:54] trainer.py:2417 >>   Total optimization steps = 6
[INFO|2025-07-15 09:38:54] trainer.py:2418 >>   Number of trainable parameters = 20,971,520
[INFO|2025-07-15 09:44:53] logging.py:143 >> {'loss': 0.7866, 'learning_rate': 6.9098e-06, 'epoch': 2.53, 'throughput': 47.58}
[INFO|2025-07-15 09:45:59] trainer.py:3993 >> Saving model checkpoint to saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53/checkpoint-6
[INFO|2025-07-15 09:45:59] configuration_utils.py:698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/config.json
[INFO|2025-07-15 09:45:59] configuration_utils.py:770 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|2025-07-15 09:45:59] tokenization_utils_base.py:2356 >> chat template saved in saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53/checkpoint-6/chat_template.jinja
[INFO|2025-07-15 09:45:59] tokenization_utils_base.py:2525 >> tokenizer config file saved in saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53/checkpoint-6/tokenizer_config.json
[INFO|2025-07-15 09:45:59] tokenization_utils_base.py:2534 >> Special tokens file saved in saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53/checkpoint-6/special_tokens_map.json
[INFO|2025-07-15 09:46:02] trainer.py:2676 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|2025-07-15 09:46:02] trainer.py:3993 >> Saving model checkpoint to saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53
[INFO|2025-07-15 09:46:03] configuration_utils.py:698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/config.json
[INFO|2025-07-15 09:46:03] configuration_utils.py:770 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|2025-07-15 09:46:03] tokenization_utils_base.py:2356 >> chat template saved in saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53/chat_template.jinja
[INFO|2025-07-15 09:46:03] tokenization_utils_base.py:2525 >> tokenizer config file saved in saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53/tokenizer_config.json
[INFO|2025-07-15 09:46:03] tokenization_utils_base.py:2534 >> Special tokens file saved in saves/Mistral-7B-v0.1/lora/train_2025-07-15-09-31-53/special_tokens_map.json
[WARNING|2025-07-15 09:46:03] logging.py:148 >> No metric eval_loss to plot.
[WARNING|2025-07-15 09:46:03] logging.py:148 >> No metric eval_accuracy to plot.
[INFO|2025-07-15 09:46:03] modelcard.py:450 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
